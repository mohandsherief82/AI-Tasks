{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70461dec",
   "metadata": {},
   "source": [
    "# Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd19ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import Caltech101\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import random\n",
    "from captum.attr import Occlusion\n",
    "import cv2 as cv\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b740dd6",
   "metadata": {},
   "source": [
    "# Phase 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d79090",
   "metadata": {},
   "source": [
    "## Data Import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9b07d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Datapath\n",
    "dataset_path = \"./caltech101\"\n",
    "\n",
    "# Transforms for normalization,  turning from PIL to tensor, resizing, transforming to RGB, and cropping\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    # Convert all images to RGB format before converting to a tensor\n",
    "    transforms.Lambda(lambda x: x.convert('RGB')),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create dataset object\n",
    "caltech_dataset = Caltech101(\n",
    "    root=dataset_path,\n",
    "    download=False,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "caltech_dataloader = DataLoader(caltech_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0fb5ed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dataset into training and validation sets\n",
    "train_size = int(0.8 * len(caltech_dataset))\n",
    "val_size = len(caltech_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(caltech_dataset, [train_size, val_size])\n",
    "\n",
    "# Load Dataset using DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87496d6f",
   "metadata": {},
   "source": [
    "## Performing the Transfer learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7e660c",
   "metadata": {},
   "source": [
    "### Load pre-trained models:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effdfcfd",
   "metadata": {},
   "source": [
    "#### Resnet-34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fcaa309",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet34 = models.resnet34(pretrained=True)\n",
    "\n",
    "# Freeze all layers except the final layer\n",
    "for param in resnet34.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the final layer for 101 classes from FC to linear\n",
    "num_ftrs = resnet34.fc.in_features\n",
    "resnet34.fc = nn.Linear(num_ftrs, 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcfacac",
   "metadata": {},
   "source": [
    "#### MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64730450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohand/Projects/Python Projects/Github Tracked/AI-Tasks/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "mobilenet_v2 = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "# Freeze all layers except the final layer\n",
    "for param in mobilenet_v2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the final layer for 101 classes from sequential to linear\n",
    "num_ftrs = mobilenet_v2.classifier[1].in_features\n",
    "mobilenet_v2.classifier[1] = nn.Linear(num_ftrs, 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba14581",
   "metadata": {},
   "source": [
    "### Train the last layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5308a87",
   "metadata": {},
   "source": [
    "#### ResNet-34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a56409a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Train Loss: 0.2165, Validation Loss: 0.3015, Validation Accuracy: 0.9159\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Number of epochs\n",
    "num_epochs = 1\n",
    "\n",
    "# Initialize Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet34.parameters(), lr=0.001)\n",
    "\n",
    "# Train the last layer\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    resnet34.train()\n",
    "    running_train_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet34(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    resnet34.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = resnet34(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_val_loss = running_val_loss / len(val_loader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Validation Loss: {avg_val_loss:.4f}, \"\n",
    "          f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33230a29",
   "metadata": {},
   "source": [
    "#### MobileNet V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e497c27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mobilenet_v2.classifier[1].parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Train the last layer\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    mobilenet_v2.train()\n",
    "    running_train_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mobilenet_v2(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    mobilenet_v2.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = mobilenet_v2(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_val_loss = running_val_loss / len(val_loader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Validation Loss: {avg_val_loss:.4f}, \"\n",
    "          f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48735674",
   "metadata": {},
   "source": [
    "# Phase 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cddbc",
   "metadata": {},
   "source": [
    "## The JSMA Attack function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09d6d91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsma_attack(model, original_image, original_label, mask_labels, target_label, theta=-0.1, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Implements the Jacobian-based Saliency Map Attack (JSMA).\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to attack.\n",
    "        original_image: The original input image.\n",
    "        original_label: The true label of the image.\n",
    "        target_label: The desired adversarial target class.\n",
    "        theta: The perturbation amount added to each pixel.\n",
    "        gamma: A scalar between 0 and 1 that controls the maximum\n",
    "                       number of pixels to modify.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The perturbed, adversarial image.\n",
    "        bool: True if the attack was successful, False otherwise.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Clone the original image and enable gradient tracking\n",
    "    adversarial_image = original_image.clone().detach()\n",
    "    adversarial_image.requires_grad = True\n",
    "    \n",
    "    # Max perturbations allowed based on gamma\n",
    "    max_perturbations = int(np.prod(original_image.shape) * epsilon)\n",
    "    \n",
    "    # Keep track of modified pixels to prevent redundant changes\n",
    "    modified_pixels = set()\n",
    "    \n",
    "    # Determine the number of classes for the Jacobian calculation\n",
    "    output = model(adversarial_image)\n",
    "    num_classes = output.shape[1]\n",
    "\n",
    "    # Check if label needs to be masked\n",
    "    if original_label in mask_labels:\n",
    "        return adversarial_image.detach(), True\n",
    "    \n",
    "    # The attack is an iterative process\n",
    "    for _ in tqdm(range(max_perturbations), desc=\"JSMA Attack Progress\"):\n",
    "        # Forward pass to get the output logits\n",
    "        output = model(adversarial_image)\n",
    "        \n",
    "        # Check if the attack has succeeded\n",
    "        if output.argmax(dim=1).item() == target_label:\n",
    "            print(\"Attack successful! Model classified as target label.\")\n",
    "            return adversarial_image.detach(), True\n",
    "\n",
    "        # Compute the Jacobian Matrix\n",
    "        # Initialize the Jacobian tensor with zeros\n",
    "        jacobian = torch.zeros(num_classes, np.prod(adversarial_image.shape))\n",
    "        \n",
    "        # For each class, compute the gradient of its logit with respect to the input image\n",
    "        for c in range(num_classes):\n",
    "            # Zero out previous gradients\n",
    "            if adversarial_image.grad is not None:\n",
    "                adversarial_image.grad.zero_()\n",
    "            \n",
    "            # Compute the gradient of the current class's output\n",
    "            output[0, c].backward(retain_graph=True)\n",
    "            \n",
    "            # Flatten the gradient and store it in the Jacobian matrix\n",
    "            jacobian[c] = adversarial_image.grad.view(-1).clone()\n",
    "\n",
    "        # Construct the Saliency Map\n",
    "        # Get the Jacobian for the target class and for all other classes\n",
    "        target_jacobian = jacobian[target_label]\n",
    "        other_jacobians = jacobian[np.arange(num_classes) != target_label].sum(dim=0)\n",
    "        \n",
    "        # Saliency map calculation based on the paper's formula\n",
    "        saliency_map = target_jacobian * (other_jacobians + target_jacobian)\n",
    "        \n",
    "        # Mask out pixels that have already been modified\n",
    "        for pixel_idx in modified_pixels:\n",
    "            saliency_map[pixel_idx] = -1 # A negative value to ensure it's not chosen\n",
    "        \n",
    "        # Find the pixel with the highest saliency score\n",
    "        pixel_to_change = torch.argmax(saliency_map)\n",
    "        \n",
    "        # If no valid pixel can be found, stop the attack\n",
    "        if saliency_map[pixel_to_change] <= 0:\n",
    "            print(\"No suitable pixels found. Attack failed.\")\n",
    "            return adversarial_image.detach(), False\n",
    "        \n",
    "        # Modify the selected pixel\n",
    "        # Add the perturbation to the selected pixel\n",
    "        adversarial_image.data.view(-1)[pixel_to_change] += theta\n",
    "        \n",
    "        # Clamp the pixel value to be within the valid range [0, 1]\n",
    "        adversarial_image.data = torch.clamp(adversarial_image.data, 0, 1)\n",
    "        \n",
    "        # Add the modified pixel to the set\n",
    "        modified_pixels.add(pixel_to_change.item())\n",
    "        \n",
    "        # Check if the adversarial image is still valid\n",
    "        if torch.equal(original_image, adversarial_image):\n",
    "            print(\"No change applied. Attack failed.\")\n",
    "            return adversarial_image.detach(), False\n",
    "            \n",
    "    # Attack failed if the loop completes without success\n",
    "    print(\"Attack failed. Maximum perturbations reached.\")\n",
    "    return adversarial_image.detach(), False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e956a3a",
   "metadata": {},
   "source": [
    "## Perform the attack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25783379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove human and car like objects from categories\n",
    "labels_to_remove = [\"Faces\", \"Faces_easy\", \"Motorbikes\", \"car_side\"]\n",
    "class_names = caltech_dataset.categories\n",
    "for c in labels_to_remove:\n",
    "    class_names.remove(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8af0420f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing a JSMA attack on the dataset...\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Set the model\n",
    "model = resnet34\n",
    "\n",
    "print(\"Performing a JSMA attack on the dataset...\\n\")\n",
    "\n",
    "# Perform the attack on each image in the dataloader\n",
    "try:\n",
    "    for image, label in iter(caltech_dataloader):\n",
    "        target_label = random.choice(class_names)\n",
    "\n",
    "        # Perform the attack\n",
    "        # Run the attack\n",
    "        adversarial_image, success = jsma_attack(\n",
    "            model=model,\n",
    "            original_image=image,\n",
    "            original_label=label,\n",
    "            target_label=target_label,\n",
    "            theta=1.0, # Perturbation strength\n",
    "            epsilon=0.2,  # Max percentage of pixels to change\n",
    "            mask_labels=labels_to_remove\n",
    "        )\n",
    "        if success:\n",
    "            print(f\"Success!!\")\n",
    "        else:\n",
    "            pass\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f9c7d0",
   "metadata": {},
   "source": [
    "# Phase 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8ff176",
   "metadata": {},
   "source": [
    "## Extra Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f893dfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_transformed_image(transformed_image_array):\n",
    "    # Reverses the transformations and displays the image using matplotlib.pyplot.\n",
    "    # Inverse the normalization\n",
    "    mean = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))\n",
    "    std = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))\n",
    "    \n",
    "    # De-normalize: (image * std) + mean\n",
    "    denormalized_image = transformed_image_array * std + mean\n",
    "    \n",
    "    # Transpose back from (C, H, W) to (H, W, C) for plotting\n",
    "    plottable_image = denormalized_image.transpose((1, 2, 0))\n",
    "    \n",
    "    # Clip values to the valid [0, 1] range to prevent issues with display\n",
    "    plottable_image = np.clip(plottable_image, 0, 1)\n",
    "    \n",
    "    # Return the original image\n",
    "    return plottable_image\n",
    "\n",
    "\n",
    "# Visualization function for Grad-CAM\n",
    "# Edge detection\n",
    "def get_canny_edge(img, threshold1=30, threshold2=80):\n",
    "    # Turn the image from RGB to Grey scale\n",
    "    gray = cv.cvtColor(img, cv.COLOR_RGB2GRAY)\n",
    "    gray *= 255\n",
    "    gray = gray.astype(np.uint8)\n",
    "\n",
    "    # Gaussian blur\n",
    "    gray = cv.GaussianBlur(gray, (7,7), 0)\n",
    "\n",
    "    # Get edge\n",
    "    edge = 255 - cv.Canny(gray, threshold1, threshold2)\n",
    "    edge = np.stack([edge]*3, axis=-1)/255\n",
    "\n",
    "    return edge\n",
    "\n",
    "\n",
    "# Plot the image and Grad-CAM\n",
    "def plot_gradcam(img, visualization, title='Grad-CAM'):\n",
    "    fig, ax = plt.subplot(1, 2, figsize=(10,10))\n",
    "\n",
    "    ax[0].imshow(img)\n",
    "    ax[0].set_title(title)\n",
    "\n",
    "    ax[1].imshow(visualization)\n",
    "    ax[1].set_title(title)\n",
    "\n",
    "    for a in ax:\n",
    "        a.set_xticks([])\n",
    "        a.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c891b6d8",
   "metadata": {},
   "source": [
    "## Occlusion based salency map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea9a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of images\n",
    "for image, label in caltech_dataloader:\n",
    "    display_image = image\n",
    "    target = label\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b190479f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6464, 3, 224, 224)\n",
      "Min: -2.085144\n",
      "Max: 1.56184\n"
     ]
    }
   ],
   "source": [
    "# Define the occlusion object\n",
    "occlusion = Occlusion(model)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(display_image)\n",
    "\n",
    "# Apply occlusion\n",
    "attributions = occlusion.attribute(\n",
    "    display_image,\n",
    "    strides=16,\n",
    "    sliding_window_shapes=(3, 16, 16)\n",
    ")\n",
    "\n",
    "# Convert to numpy and plot the saliency map\n",
    "saliency_map = attributions.squeeze().cpu().detach().numpy()\n",
    "print(saliency_map.shape) # (3, 224, 224)\n",
    "\n",
    "# Select one channel as they are all the same\n",
    "saliency_map = saliency_map[0]\n",
    "\n",
    "print(\"Min:\", saliency_map.min())\n",
    "print(\"Max:\", saliency_map.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c3ef8787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5379/4070401720.py:1: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  show_transformed_image(np.array(display_image[0, 0]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[1.        , 0.95975546, 0.91200436],\n",
       "        [1.        , 0.95975546, 0.91200436],\n",
       "        [1.        , 0.95975546, 0.91200436],\n",
       "        ...,\n",
       "        [1.        , 0.95975546, 0.91200436],\n",
       "        [1.        , 0.95975546, 0.91200436],\n",
       "        [1.        , 0.95975546, 0.91200436]],\n",
       "\n",
       "       [[1.        , 0.95975546, 0.91200436],\n",
       "        [1.        , 0.95975546, 0.91200436],\n",
       "        [1.        , 0.95975546, 0.91200436],\n",
       "        ...,\n",
       "        [1.        , 0.95975546, 0.91200436],\n",
       "        [1.        , 0.95975546, 0.91200436],\n",
       "        [1.        , 0.95975546, 0.91200436]],\n",
       "\n",
       "       [[1.        , 0.95975546, 0.91200436],\n",
       "        [1.        , 0.95975546, 0.91200436],\n",
       "        [1.        , 0.95975546, 0.91200436],\n",
       "        ...,\n",
       "        [1.        , 0.95975546, 0.91200436],\n",
       "        [1.        , 0.95975546, 0.91200436],\n",
       "        [1.        , 0.95975546, 0.91200436]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.        , 0.95975546, 0.91200436],\n",
       "        [1.        , 0.95975546, 0.91200436],\n",
       "        [1.        , 0.95975546, 0.91200436],\n",
       "        ...,\n",
       "        [1.        , 0.95975546, 0.91200436],\n",
       "        [1.        , 0.95975546, 0.91200436],\n",
       "        [1.        , 0.95975546, 0.91200436]],\n",
       "\n",
       "       [[1.        , 0.95975546, 0.91200436],\n",
       "        [1.        , 0.95975546, 0.91200436],\n",
       "        [1.        , 0.95975546, 0.91200436],\n",
       "        ...,\n",
       "        [1.        , 0.95975546, 0.91200436],\n",
       "        [1.        , 0.95975546, 0.91200436],\n",
       "        [1.        , 0.95975546, 0.91200436]],\n",
       "\n",
       "       [[1.        , 0.95975546, 0.91200436],\n",
       "        [1.        , 0.95975546, 0.91200436],\n",
       "        [1.        , 0.95975546, 0.91200436],\n",
       "        ...,\n",
       "        [1.        , 0.95975546, 0.91200436],\n",
       "        [1.        , 0.95975546, 0.91200436],\n",
       "        [1.        , 0.95975546, 0.91200436]]], shape=(224, 224, 3))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_transformed_image(np.array(display_image[0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435aba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate for images in batch\n",
    "for image in display_image:\n",
    "    image = np.round(np.clip(np.array(image * 255), 0, 255))\n",
    "    # Create edge image\n",
    "    grey_image = cv.cvtColor(image, cv.COLOR_RGB2GRAY)\n",
    "    edge_image = cv.Canny(grey_image, 50, 100)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Display image and prediction\n",
    "    ax[0].imshow(image)\n",
    "    ax[0].axvline(x=target, color='r',label='Target')\n",
    "    ax[0].axvline(x=pred, color='g',label='Prediction')\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[0].set_yticks([])\n",
    "    ax[0].set_xlabel('Direction',size=15)\n",
    "\n",
    "    # Display saliency map\n",
    "    ax[1].imshow(edge_image , cmap='gray')\n",
    "    ax[1].set_xticks([])\n",
    "    ax[1].set_yticks([])\n",
    "\n",
    "    # Normalize saliency map values\n",
    "    abs_max = np.max(np.abs(saliency_map)) \n",
    "\n",
    "    # Overlay saliency map with color bar\n",
    "    saliency_plot = ax[1].imshow(saliency_map, \n",
    "                                alpha=0.9,\n",
    "                                cmap='coolwarm', \n",
    "                                vmin=-abs_max, \n",
    "                                vmax=abs_max)\n",
    "\n",
    "    # Create colorbar\n",
    "    cbar = fig.colorbar(saliency_plot, ax=ax[1], fraction=0.046, pad=0.04)\n",
    "    cbar.set_label(\"Saliency Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20551915",
   "metadata": {},
   "source": [
    "## Grad-CAM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5f71a057",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Get the GradCAM heatmap\u001b[39;00m\n\u001b[32m      9\u001b[39m cam = GradCAM(model=model, target_layers=target_layer)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m heatmap = \u001b[43mcam\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Plot the heatmap and the image\u001b[39;00m\n\u001b[32m     13\u001b[39m edge = get_canny_edge(image)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python Projects/Github Tracked/AI-Tasks/.venv/lib/python3.12/site-packages/pytorch_grad_cam/base_cam.py:209\u001b[39m, in \u001b[36mBaseCAM.__call__\u001b[39m\u001b[34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m aug_smooth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.forward_augmentation_smoothing(input_tensor, targets, eigen_smooth)\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python Projects/Github Tracked/AI-Tasks/.venv/lib/python3.12/site-packages/pytorch_grad_cam/base_cam.py:109\u001b[39m, in \u001b[36mBaseCAM.forward\u001b[39m\u001b[34m(self, input_tensor, targets, eigen_smooth)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.uses_gradients:\n\u001b[32m    108\u001b[39m     \u001b[38;5;28mself\u001b[39m.model.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     loss = \u001b[38;5;28msum\u001b[39m([\u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m target, output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(targets, outputs)])\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.detach:\n\u001b[32m    111\u001b[39m         loss.backward(retain_graph=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "# Set model to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Define target layer and class\n",
    "# Specific to ResNet-34\n",
    "target_layer = [model.layer4]\n",
    "\n",
    "# Get the GradCAM heatmap\n",
    "cam = GradCAM(model=model, target_layers=target_layer)\n",
    "heatmap = cam(input_tensor=image.squeeze().cpu().detach(), targets=target)\n",
    "\n",
    "# Plot the heatmap and the image\n",
    "edge = get_canny_edge(image)\n",
    "visulaization = show_cam_on_image(edge, heatmap[0], use_rgb=True)\n",
    "\n",
    "plot_gradcam(image, visulaization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bcd6b8",
   "metadata": {},
   "source": [
    "# Phase 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf35290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0243fc4f",
   "metadata": {},
   "source": [
    "# Phase 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f392684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
