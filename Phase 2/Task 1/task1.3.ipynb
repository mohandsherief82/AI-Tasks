{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6fffa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import Caltech101\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0094cf4",
   "metadata": {},
   "source": [
    "# Phase 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a941a3",
   "metadata": {},
   "source": [
    "## Data Import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e80666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Datapath\n",
    "dataset_path = \"./caltech101\"\n",
    "\n",
    "# Transforms for normalization,  turning from PIL to tensor, resizing, transforming to RGB, and cropping\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    # Convert all images to RGB format before converting to a tensor\n",
    "    transforms.Lambda(lambda x: x.convert('RGB')),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224,    for image, label in iter(caltech_dataloader):\n",
    "        target_label = random.choice(class_names)\n",
    "\n",
    "        # Perform the attack\n",
    "        # Run the attack\n",
    "        adversarial_image, success = jsma_attack(\n",
    "            model=model,\n",
    "            original_image=image,\n",
    "            original_label=label,\n",
    "            target_label=target_label,\n",
    "            theta=1.0, # Perturbation strength\n",
    "            epsilon=0.2,  # Max percentage of pixels to change\n",
    "            mask_labels=labels_to_remove\n",
    "        )\n",
    "        if success:\n",
    "            print(f\"Success!!\")\n",
    "        else:\n",
    "            pass 0.225])\n",
    "])\n",
    "\n",
    "# Create dataset object\n",
    "caltech_dataset =Caltech101(\n",
    "    root=dataset_path,\n",
    "    download=False,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "caltech_dataloader = DataLoader(caltech_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f930de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dataset into training and validation sets\n",
    "train_size = int(0.8 * len(caltech_dataset))\n",
    "val_size = len(caltech_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(caltech_dataset, [train_size, val_size])\n",
    "\n",
    "# Load Dataset using DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33ab4e4",
   "metadata": {},
   "source": [
    "## Performing the Transfer learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9ccb1e",
   "metadata": {},
   "source": [
    "### Load pre-trained models:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5afccbd",
   "metadata": {},
   "source": [
    "#### Resnet-34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad01e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet34 = models.resnet34(pretrained=True)\n",
    "\n",
    "# Freeze all layers except the final layer\n",
    "for param in resnet34.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the final layer for 101 classes from FC to linear\n",
    "num_ftrs = resnet34.fc.in_features\n",
    "resnet34.fc = nn.Linear(num_ftrs, 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab5855f",
   "metadata": {},
   "source": [
    "#### MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da620e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_v2 = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "# Freeze all layers except the final layer\n",
    "for param in mobilenet_v2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the final layer for 101 classes from sequential to linear\n",
    "num_ftrs = mobilenet_v2.classifier[1].in_features\n",
    "mobilenet_v2.classifier[1] = nn.Linear(num_ftrs, 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06abfe1e",
   "metadata": {},
   "source": [
    "### Train the last layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2618230f",
   "metadata": {},
   "source": [
    "#### ResNet-34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1c4847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Initialize Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet34.parameters(), lr=0.001)\n",
    "\n",
    "# Train the last layer\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    resnet34.train()\n",
    "    running_train_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet34(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    resnet34.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = resnet34(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_val_loss = running_val_loss / len(val_loader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Validation Loss: {avg_val_loss:.4f}, \"\n",
    "          f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6f0b05",
   "metadata": {},
   "source": [
    "#### MobileNet V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e2e967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mobilenet_v2.classifier[1].parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Train the last layer\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    mobilenet_v2.train()\n",
    "    running_train_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mobilenet_v2(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    mobilenet_v2.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = mobilenet_v2(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_val_loss = running_val_loss / len(val_loader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Validation Loss: {avg_val_loss:.4f}, \"\n",
    "          f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3211bf",
   "metadata": {},
   "source": [
    "# Phase 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8def308d",
   "metadata": {},
   "source": [
    "## The JSMA Attack function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "922104d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsma_attack(model, original_image, original_label, mask_labels, target_label, theta=-0.1, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Implements the Jacobian-based Saliency Map Attack (JSMA).\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to attack.\n",
    "        original_image: The original input image.\n",
    "        original_label: The true label of the image.\n",
    "        target_label: The desired adversarial target class.\n",
    "        theta: The perturbation amount added to each pixel.\n",
    "        gamma: A scalar between 0 and 1 that controls the maximum\n",
    "                       number of pixels to modify.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The perturbed, adversarial image.\n",
    "        bool: True if the attack was successful, False otherwise.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Clone the original image and enable gradient tracking\n",
    "    adversarial_image = original_image.clone().detach()\n",
    "    adversarial_image.requires_grad = True\n",
    "    \n",
    "    # Max perturbations allowed based on gamma\n",
    "    max_perturbations = int(np.prod(original_image.shape) * epsilon)\n",
    "    \n",
    "    # Keep track of modified pixels to prevent redundant changes\n",
    "    modified_pixels = set()\n",
    "    \n",
    "    # Determine the number of classes for the Jacobian calculation\n",
    "    output = model(adversarial_image)\n",
    "    num_classes = output.shape[1]\n",
    "\n",
    "    # Check if label needs to be masked\n",
    "    if original_label in mask_labels:\n",
    "        return adversarial_image.detach(), True\n",
    "    \n",
    "    # The attack is an iterative process\n",
    "    for _ in tqdm(range(max_perturbations), desc=\"JSMA Attack Progress\"):\n",
    "        # Forward pass to get the output logits\n",
    "        output = model(adversarial_image)\n",
    "        \n",
    "        # Check if the attack has succeeded\n",
    "        if output.argmax(dim=1).item() == target_label:\n",
    "            print(\"Attack successful! Model classified as target label.\")\n",
    "            return adversarial_image.detach(), True\n",
    "\n",
    "        # Compute the Jacobian Matrix\n",
    "        # Initialize the Jacobian tensor with zeros\n",
    "        jacobian = torch.zeros(num_classes, np.prod(adversarial_image.shape))\n",
    "        \n",
    "        # For each class, compute the gradient of its logit with respect to the input image\n",
    "        for c in range(num_classes):\n",
    "            # Zero out previous gradients\n",
    "            if adversarial_image.grad is not None:\n",
    "                adversarial_image.grad.zero_()\n",
    "            \n",
    "            # Compute the gradient of the current class's output\n",
    "            output[0, c].backward(retain_graph=True)\n",
    "            \n",
    "            # Flatten the gradient and store it in the Jacobian matrix\n",
    "            jacobian[c] = adversarial_image.grad.view(-1).clone()\n",
    "\n",
    "        # Construct the Saliency Map\n",
    "        # Get the Jacobian for the target class and for all other classes\n",
    "        target_jacobian = jacobian[target_label]\n",
    "        other_jacobians = jacobian[np.arange(num_classes) != target_label].sum(dim=0)\n",
    "        \n",
    "        # Saliency map calculation based on the paper's formula\n",
    "        saliency_map = target_jacobian * (other_jacobians + target_jacobian)\n",
    "        \n",
    "        # Mask out pixels that have already been modified\n",
    "        for pixel_idx in modified_pixels:\n",
    "            saliency_map[pixel_idx] = -1 # A negative value to ensure it's not chosen\n",
    "        \n",
    "        # Find the pixel with the highest saliency score\n",
    "        pixel_to_change = torch.argmax(saliency_map)\n",
    "        \n",
    "        # If no valid pixel can be found, stop the attack\n",
    "        if saliency_map[pixel_to_change] <= 0:\n",
    "            print(\"No suitable pixels found. Attack failed.\")\n",
    "            return adversarial_image.detach(), False\n",
    "        \n",
    "        # Modify the selected pixel\n",
    "        # Add the perturbation to the selected pixel\n",
    "        adversarial_image.data.view(-1)[pixel_to_change] += theta\n",
    "        \n",
    "        # Clamp the pixel value to be within the valid range [0, 1]\n",
    "        adversarial_image.data = torch.clamp(adversarial_image.data, 0, 1)\n",
    "        \n",
    "        # Add the modified pixel to the set\n",
    "        modified_pixels.add(pixel_to_change.item())\n",
    "        \n",
    "        # Check if the adversarial image is still valid\n",
    "        if torch.equal(original_image, adversarial_image):\n",
    "            print(\"No change applied. Attack failed.\")\n",
    "            return adversarial_image.detach(), False\n",
    "            \n",
    "    # Attack failed if the loop completes without success\n",
    "    print(\"Attack failed. Maximum perturbations reached.\")\n",
    "    return adversarial_image.detach(), False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a977d6bd",
   "metadata": {},
   "source": [
    "## Perform the attack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37750c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faces\n"
     ]
    }
   ],
   "source": [
    "for c in caltech_dataset.categories:\n",
    "    if c == \"Faces_easy\":\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dda2093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing a JSMA attack on the dataset...\n",
      "\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Set the model\n",
    "model = resnet34\n",
    "\n",
    "print(\"Performing a JSMA attack on the dataset...\\n\")\n",
    "\n",
    "# Remove human and car like\n",
    "labels_to_remove = [\"Faces\", \"Faces_easy\", \"Motorbikes\", \"car_side\"]\n",
    "class_names = caltech_dataset.categories\n",
    "for c in labels_to_remove:\n",
    "    class_names.remove(c)\n",
    "\n",
    "# Perform the attack on each image in the dataloader\n",
    "try:\n",
    "    for image, label in iter(caltech_dataloader):\n",
    "        target_label = random.choice(class_names)\n",
    "\n",
    "        # Perform the attack\n",
    "        # Run the attack\n",
    "        adversarial_image, success = jsma_attack(\n",
    "            model=model,\n",
    "            original_image=image,\n",
    "            original_label=label,\n",
    "            target_label=target_label,\n",
    "            theta=1.0, # Perturbation strength\n",
    "            epsilon=0.2,  # Max percentage of pixels to change\n",
    "            mask_labels=labels_to_remove\n",
    "        )\n",
    "        if success:\n",
    "            print(f\"Success!!\")\n",
    "        else:\n",
    "            pass\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f2a1f2",
   "metadata": {},
   "source": [
    "# Phase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174063b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d99b5b92",
   "metadata": {},
   "source": [
    "# Phase 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0599305c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a06f3fea",
   "metadata": {},
   "source": [
    "# Phase 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570aa0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
