# Report:

## Attack methods:

- The type of Adversarial Attack used for attacking the system is the Maximal Jacobian-based Saliency Map Attack(JSMA).
- This type of attack works by adding noise at random positions to the image by maximizing or minimizing pixel values which can fool the classification model.
- It can work by specifiying a specific class to misclassify or not specify whether to increase or decrease the pixel value.
- It takse the saliency map, that shows pixels with the most importance for classification, and change the values of the most important pixels 
